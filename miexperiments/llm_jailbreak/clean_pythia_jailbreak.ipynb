{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f9mTHIbQ-RaP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (0.6.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: jaxtyping in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (0.2.19)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from jaxtyping) (1.24.4)\n",
            "Requirement already satisfied: typeguard>=2.13.3 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from jaxtyping) (4.1.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from jaxtyping) (4.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from typeguard>=2.13.3->jaxtyping) (5.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from importlib-metadata>=3.6->typeguard>=2.13.3->jaxtyping) (3.16.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (4.32.0)\n",
            "Requirement already satisfied: fschat in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (0.2.27)\n",
            "Requirement already satisfied: livelossplot in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (0.5.5)\n",
            "Requirement already satisfied: filelock in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (3.12.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (2023.8.8)\n",
            "Requirement already satisfied: requests in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (3.8.5)\n",
            "Requirement already satisfied: fastapi in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (0.103.1)\n",
            "Requirement already satisfied: httpx in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (0.24.1)\n",
            "Requirement already satisfied: markdown2[all] in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (2.4.10)\n",
            "Requirement already satisfied: nh3 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (0.2.14)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (3.0.39)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (1.10.12)\n",
            "Requirement already satisfied: rich>=10.0.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (13.5.2)\n",
            "Requirement already satisfied: shortuuid in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (1.0.11)\n",
            "Requirement already satisfied: tiktoken in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (0.4.0)\n",
            "Requirement already satisfied: uvicorn in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat) (0.23.2)\n",
            "Requirement already satisfied: matplotlib in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from livelossplot) (3.7.2)\n",
            "Requirement already satisfied: bokeh in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from livelossplot) (3.1.1)\n",
            "Requirement already satisfied: fsspec in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: wcwidth in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from prompt-toolkit>=3.0.0->fschat) (0.2.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from rich>=10.0.0->fschat) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from rich>=10.0.0->fschat) (2.16.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat) (1.3.1)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from bokeh->livelossplot) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from bokeh->livelossplot) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from bokeh->livelossplot) (2.0.3)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from bokeh->livelossplot) (9.4.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from bokeh->livelossplot) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from bokeh->livelossplot) (2023.7.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fastapi->fschat) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fastapi->fschat) (0.27.0)\n",
            "Requirement already satisfied: certifi in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat) (0.17.3)\n",
            "Requirement already satisfied: idna in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat) (3.4)\n",
            "Requirement already satisfied: sniffio in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat) (1.3.0)\n",
            "Requirement already satisfied: wavedrom in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from markdown2[all]->fschat) (2.0.3.post3)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from matplotlib->livelossplot) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from matplotlib->livelossplot) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from matplotlib->livelossplot) (1.4.5)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from matplotlib->livelossplot) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from matplotlib->livelossplot) (6.0.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from uvicorn->fschat) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from uvicorn->fschat) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->fschat) (1.1.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->livelossplot) (3.16.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat) (0.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "Requirement already satisfied: svgwrite in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from wavedrom->markdown2[all]->fschat) (1.4.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# sometime jaxtyping doesn't install correctly,\n",
        "# just restart and try again in that case (:\n",
        "%pip install einops\n",
        "%pip install jaxtyping\n",
        "%pip install transformers fschat livelossplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, GPTNeoXForCausalLM\n",
        "from fastchat.conversation import Conversation, get_conv_template\n",
        "from livelossplot import PlotLosses\n",
        "from jaxtyping import Int, Float\n",
        "from typing import Tuple, List, Optional, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osKRtqTw-BRh",
        "outputId": "8a82cffd-e63f-4248-81b0-1422cb05c017"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/rp/dmbbthp17vdc__2336dxqvvr0000gn/T/ipykernel_16107/752335641.py:7: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"load_ext autoreload\")\n",
            "/var/folders/rp/dmbbthp17vdc__2336dxqvvr0000gn/T/ipykernel_16107/752335641.py:8: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"autoreload 2\")\n"
          ]
        }
      ],
      "source": [
        "# from suffix_attack import *\n",
        "import torch\n",
        "import einops\n",
        "import gc\n",
        "\n",
        "ipython = get_ipython()\n",
        "ipython.magic(\"load_ext autoreload\")\n",
        "ipython.magic(\"autoreload 2\")\n",
        "\n",
        "def clear_gpu_mem():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_2mV156-yM3"
      },
      "source": [
        "### The code for the suffix attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DFep047i-b_i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, GPTNeoXForCausalLM\n",
        "from fastchat.conversation import Conversation, get_conv_template\n",
        "from livelossplot import PlotLosses\n",
        "from jaxtyping import Int, Float\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "\n",
        "device = \"cpu\"\n",
        "non_mps_device = \"cpu\"\n",
        "model_name = \"pythia\"\n",
        "auth_token = \"\"\n",
        "precision = torch.float32\n",
        "\n",
        "class SuffixManager:\n",
        "    def __init__(self, *, tokenizer, conv_template: Conversation, instruction: str, target: str, adv_string: List[int]):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.conv_template = conv_template\n",
        "        self.instruction = instruction\n",
        "        self.target = target\n",
        "        self.adv_string = adv_string\n",
        "\n",
        "    def get_prompt(self, adv_string=None):\n",
        "\n",
        "        if adv_string is not None:\n",
        "            self.adv_string = adv_string\n",
        "\n",
        "        self.conv_template.append_message(self.conv_template.roles[0], f\"{self.instruction} {self.adv_string}\")\n",
        "        self.conv_template.append_message(self.conv_template.roles[1], f\"{self.target}\")\n",
        "        prompt = self.conv_template.get_prompt()\n",
        "\n",
        "        encoding = self.tokenizer(prompt)\n",
        "        toks = encoding.input_ids\n",
        "\n",
        "        if self.conv_template.name == 'llama-2':\n",
        "            self.conv_template.messages = []\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[0], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._user_role_slice = slice(None, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._goal_slice = slice(self._user_role_slice.stop, max(self._user_role_slice.stop, len(toks)))\n",
        "\n",
        "            separator = ' ' if self.instruction else ''\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}{separator}{self.adv_string}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._control_slice = slice(self._goal_slice.stop, len(toks))\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[1], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._assistant_role_slice = slice(self._control_slice.stop, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.target}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._target_slice = slice(self._assistant_role_slice.stop, len(toks)-2)\n",
        "            self._loss_slice = slice(self._assistant_role_slice.stop-1, len(toks)-3)\n",
        "\n",
        "        elif self.conv_template.name == 'oasst_pythia':\n",
        "            # This is specific to the vicuna and pythia tokenizer and conversation prompt.\n",
        "            # It will not work with other tokenizers or prompts.\n",
        "            self.conv_template.messages = []\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[0], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._user_role_slice = slice(None, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._goal_slice = slice(self._user_role_slice.stop, max(self._user_role_slice.stop, len(toks)-1))\n",
        "\n",
        "            separator = ' ' if self.instruction else ''\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}{separator}{self.adv_string}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._control_slice = slice(self._goal_slice.stop, len(toks)-1)\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[1], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._assistant_role_slice = slice(self._control_slice.stop, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.target}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._target_slice = slice(self._assistant_role_slice.stop, len(toks)-1)\n",
        "            self._loss_slice = slice(self._assistant_role_slice.stop-1, len(toks)-2)\n",
        "        else:\n",
        "            assert False, \"Conversation template not supported\"\n",
        "\n",
        "        self.conv_template.messages = []\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def get_input_ids(self, adv_string=None):\n",
        "        prompt = self.get_prompt(adv_string=adv_string)\n",
        "        toks = self.tokenizer(prompt).input_ids\n",
        "        input_ids = torch.tensor(toks[:self._target_slice.stop])\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "    # def get_batch_input_ids(self, adv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# support for different models:\n",
        "# 1) A very small one such as pythia 70m to do fast stuff\n",
        "# 2) Llama2\n",
        "# 3) Ideally an arbitrary hf model with the same tokenizer as llama\n",
        "def get_model_and_tokenizer(model_path=\"EleutherAI/pythia-1.4b\", tokenizer_path=\"EleutherAI/pythia-1.4b\", **kwargs):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=precision,\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=auth_token\n",
        "    ).to(device)\n",
        "\n",
        "    if 'llama-2' in model_path:\n",
        "        use_fast = False\n",
        "    elif 'pythia' in model_path:\n",
        "        use_fast = True\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        tokenizer_path,\n",
        "        trust_remote_code=True,\n",
        "        use_fast=use_fast\n",
        "    )\n",
        "\n",
        "    # I don't really know why we do this\n",
        "    if 'llama-2' in model_path:\n",
        "        tokenizer.pad_token = tokenizer.unk_token\n",
        "        tokenizer.padding_side = 'left'\n",
        "    elif 'pythia' in model_path:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def token_gradients(model, input_ids, input_slice, target_slice, loss_slice):\n",
        "    if isinstance(model, LlamaForCausalLM):\n",
        "        embed_weights = model.model.embed_tokens.weight\n",
        "        embeds = model.model.embed_tokens(input_ids).detach()\n",
        "    elif isinstance(model, GPTNeoXForCausalLM):\n",
        "        embed_weights = model.base_model.embed_in.weight\n",
        "        embeds = model.base_model.embed_in(input_ids.unsqueeze(0).to(device)).detach()\n",
        "    else:\n",
        "        assert False, \"Only Llama and GPTNeoX models are supported\"\n",
        "\n",
        "    one_hot = torch.zeros(\n",
        "        input_ids[input_slice].shape[0],\n",
        "        embed_weights.shape[0],\n",
        "        dtype=embed_weights.dtype\n",
        "    ).to(device)\n",
        "\n",
        "    # maybe use scatter here?\n",
        "    one_hot[torch.arange(input_ids[input_slice].shape[0]),input_ids[input_slice]] = 1.0\n",
        "\n",
        "    # this returns an error when run on MPS\n",
        "    #one_hot = torch.zeros(\n",
        "    #    input_ids[input_slice].shape[0],\n",
        "    #    embed_weights.shape[0],\n",
        "    #    device='cpu',\n",
        "    #    dtype=embed_weights.dtype\n",
        "    #)\n",
        "    #one_hot = one_hot_cpu.scatter(\n",
        "    #    1,\n",
        "    #    input_ids[input_slice].unsqueeze(1),\n",
        "    #    torch.ones(one_hot_cpu.shape[0], 1, device=model.device, dtype=embed_weights.dtype)\n",
        "    #)\n",
        "\n",
        "    #print(f\"One-hot shape is {one_hot.shape}\")\n",
        "    #print(f\"Embed matrix has shape {embed_weights.shape}\")\n",
        "\n",
        "    one_hot.requires_grad_()\n",
        "    #input_embeds = (one_hot @ embed_weights).unsqueeze(0)\n",
        "\n",
        "    #print(f\"Input embeds were computed\")\n",
        "\n",
        "    # now stitch it together with the rest of the embeddings\n",
        "    #embeds = get_embeddings(model, input_ids.unsqueeze(0)).detach()\n",
        "\n",
        "\n",
        "    full_embeds = torch.cat(\n",
        "        [\n",
        "            embeds[:,:input_slice.start,:],\n",
        "            (one_hot @ embed_weights).unsqueeze(0),\n",
        "            embeds[:,input_slice.stop:,:]\n",
        "        ],\n",
        "        dim=1\n",
        "    ).to(device)\n",
        "\n",
        "    #print(f\"Stiched embeddings\")\n",
        "\n",
        "    #print(f\"Stiched embeddings have type {full_embeds.dtype}\")\n",
        "    #print(f\"Should have type {embeds.dtype}\")\n",
        "\n",
        "    #print(f\"Shape of full embeds is: {full_embeds.shape}\")\n",
        "\n",
        "    logits = model(inputs_embeds=full_embeds).logits\n",
        "\n",
        "    #print(\"Compute the forward pass\")\n",
        "\n",
        "    #logits = model(inputs_embeds=full_embeds).logits\n",
        "\n",
        "    #print(f\"Gotten logits {logits.shape}\")\n",
        "\n",
        "    targets = input_ids[target_slice].to(device)\n",
        "\n",
        "    loss = nn.CrossEntropyLoss()(logits[0,loss_slice,:], targets)\n",
        "\n",
        "    #print(f\"Forward pass\")\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    grad = one_hot.grad.clone()\n",
        "    grad = grad / grad.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # should we do this?\n",
        "    model.zero_grad()\n",
        "\n",
        "    return grad\n",
        "\n",
        "\n",
        "# selects k tokens within the topk (allowed) control tokens\n",
        "def sample_control(\n",
        "    control_toks,\n",
        "    grad,\n",
        "    batch_size,\n",
        "    topk=256,\n",
        "    temp=1,\n",
        "    not_allowed_tokens=None\n",
        "):\n",
        "\n",
        "    if not_allowed_tokens is not None:\n",
        "        grad[:, not_allowed_tokens.to(grad.device)] = np.infty\n",
        "\n",
        "    top_indices = (-grad).topk(topk, dim=1).indices\n",
        "    control_toks = control_toks.to(grad.device)\n",
        "\n",
        "    original_control_toks = control_toks.repeat(batch_size, 1)\n",
        "    new_token_pos = torch.arange(\n",
        "        0,\n",
        "        len(control_toks),\n",
        "        len(control_toks) / batch_size,\n",
        "        device=grad.device\n",
        "    ).type(torch.int64)\n",
        "    new_token_val = torch.gather(\n",
        "        top_indices[new_token_pos], 1,\n",
        "        torch.randint(0, topk, (batch_size, 1),\n",
        "        device=grad.device)\n",
        "    )\n",
        "    new_control_toks = original_control_toks.scatter_(1, new_token_pos.unsqueeze(-1), new_token_val)\n",
        "\n",
        "    return new_control_toks\n",
        "\n",
        "# control_cand - are the topk loss minimizing replacements\n",
        "# we found for every token in the control slice\n",
        "def get_filtered_cands(\n",
        "    tokenizer,\n",
        "    control_cand: Int[Tensor, \"batch_size\"],\n",
        "    filter_cand=True,\n",
        "    curr_control=None\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Filters invalid candidates from a set of control codes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : Tokenizer\n",
        "        Tokenizer to decode candidates.\n",
        "    control_cand : torch.Tensor\n",
        "        Candidate control codes.\n",
        "    filter_cand : bool, optional\n",
        "        Whether to filter candidates.\n",
        "    curr_control : str, optional\n",
        "        Current control code.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        List of filtered valid candidate strings.\n",
        "    \"\"\"\n",
        "    cands, count = [], 0\n",
        "    for i in range(control_cand.shape[0]):\n",
        "        decoded_str = tokenizer.decode(control_cand[i], skip_special_tokens=True)\n",
        "        if filter_cand:\n",
        "            if decoded_str != curr_control and len(tokenizer(decoded_str, add_special_tokens=False).input_ids) == len(control_cand[i]):\n",
        "                cands.append(decoded_str)\n",
        "            else:\n",
        "                count += 1\n",
        "        else:\n",
        "            cands.append(decoded_str)\n",
        "\n",
        "    #print(f\"Cands are {cands}\")\n",
        "    #print(f\"Counts are {count}\")\n",
        "\n",
        "    if filter_cand:\n",
        "        cands = cands + [cands[-1]] * (len(control_cand) - len(cands))\n",
        "        # print(f\"Warning: {round(count / len(control_cand), 2)} control candidates were not valid\")\n",
        "    return cands\n",
        "\n",
        "\n",
        "# input_ids - original input tokens\n",
        "# control_slice - the slice of input ids to replace (like the I in the paper)\n",
        "# test_controls - control code strings to substitute? supposedly the str tokens?\n",
        "# return_ids - whether to return or not the modified ids of the new tokens\n",
        "def get_logits(*, model, tokenizer, input_ids, control_slice, test_controls=None, return_ids=False, batch_size=512):\n",
        "    \"\"\"\n",
        "    Evaluates model on inputs with injected control codes.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model to evaluate.\n",
        "    tokenizer : Tokenizer\n",
        "        Tokenizer to encode controls.\n",
        "    input_ids : torch.Tensor\n",
        "        Original input IDs.\n",
        "    control_slice : slice\n",
        "        Slice of input to replace.\n",
        "    test_controls : list, optional\n",
        "        Control codes to inject.\n",
        "    return_ids : bool, optional\n",
        "        Whether to return modified IDs.\n",
        "    batch_size : int, optional\n",
        "        Evaluation batch size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        Logits from model on modified inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(test_controls[0], str):\n",
        "        max_len = control_slice.stop - control_slice.start\n",
        "        test_ids = [\n",
        "            torch.tensor(tokenizer(control, add_special_tokens=False).input_ids[:max_len], device=non_mps_device)\n",
        "            for control in test_controls\n",
        "        ]\n",
        "        pad_tok = 0\n",
        "        while pad_tok in input_ids or any([pad_tok in ids for ids in test_ids]):\n",
        "            pad_tok += 1\n",
        "        nested_ids = torch.nested.nested_tensor(test_ids)\n",
        "        test_ids = torch.nested.to_padded_tensor(nested_ids, pad_tok, (len(test_ids), max_len))\n",
        "    else:\n",
        "        raise ValueError(f\"test_controls must be a list of strings, got {type(test_controls)}\")\n",
        "\n",
        "    if not(test_ids[0].shape[0] == control_slice.stop - control_slice.start):\n",
        "        raise ValueError((\n",
        "            f\"test_controls must have shape \"\n",
        "            f\"(n, {control_slice.stop - control_slice.start}), \"\n",
        "            f\"got {test_ids.shape}\"\n",
        "        ))\n",
        "\n",
        "\n",
        "    locs = torch.arange(control_slice.start, control_slice.stop).repeat(test_ids.shape[0], 1).to(model.device)\n",
        "    ids = torch.scatter(\n",
        "        input_ids.unsqueeze(0).repeat(test_ids.shape[0], 1).to(model.device),\n",
        "        1,\n",
        "        locs,\n",
        "        test_ids\n",
        "    )\n",
        "    if pad_tok >= 0:\n",
        "        attn_mask = (ids != pad_tok).type(ids.dtype)\n",
        "    else:\n",
        "        attn_mask = None\n",
        "\n",
        "    if return_ids:\n",
        "        del locs, test_ids ; gc.collect()\n",
        "        return forward(model=model, input_ids=ids, attention_mask=attn_mask, batch_size=batch_size), ids\n",
        "    else:\n",
        "        del locs, test_ids\n",
        "        logits = forward(model=model, input_ids=ids, attention_mask=attn_mask, batch_size=batch_size)\n",
        "        del ids ; gc.collect()\n",
        "        return logits\n",
        "\n",
        "\n",
        "def target_loss(logits, ids, target_slice):\n",
        "    crit = nn.CrossEntropyLoss(reduction='none')\n",
        "    loss_slice = slice(target_slice.start-1, target_slice.stop-1)\n",
        "    loss = crit(logits[:,loss_slice,:].transpose(1,2), ids[:,target_slice])\n",
        "    return loss.mean(dim=-1)\n",
        "\n",
        "\n",
        "# what is the \"*\"?\n",
        "def forward(*, model, input_ids, attention_mask, batch_size=512):\n",
        "\n",
        "    logits = []\n",
        "    for i in range(0, input_ids.shape[0], batch_size):\n",
        "\n",
        "        batch_input_ids = input_ids[i:i+batch_size]\n",
        "        if attention_mask is not None:\n",
        "            batch_attention_mask = attention_mask[i:i+batch_size]\n",
        "        else:\n",
        "            batch_attention_mask = None\n",
        "\n",
        "        logits.append(model(input_ids=batch_input_ids, attention_mask=batch_attention_mask).logits)\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    del batch_input_ids, batch_attention_mask\n",
        "\n",
        "    return torch.cat(logits, dim=0)\n",
        "\n",
        "\n",
        "class SuffixManager:\n",
        "    def __init__(self, tokenizer, conv_template: Conversation, instruction: str, target: str, adv_string: List[int]):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.conv_template = conv_template\n",
        "        self.instruction = instruction\n",
        "        self.target = target\n",
        "        self.adv_string = adv_string\n",
        "\n",
        "    def get_prompt(self, adv_string=None):\n",
        "\n",
        "        if adv_string is not None:\n",
        "            self.adv_string = adv_string\n",
        "\n",
        "        self.conv_template.append_message(self.conv_template.roles[0], f\"{self.instruction} {self.adv_string}\")\n",
        "        self.conv_template.append_message(self.conv_template.roles[1], f\"{self.target}\")\n",
        "        prompt = self.conv_template.get_prompt()\n",
        "\n",
        "        encoding = self.tokenizer(prompt)\n",
        "        toks = encoding.input_ids\n",
        "\n",
        "        if self.conv_template.name == 'llama-2':\n",
        "            self.conv_template.messages = []\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[0], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._user_role_slice = slice(None, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._goal_slice = slice(self._user_role_slice.stop, max(self._user_role_slice.stop, len(toks)))\n",
        "\n",
        "            separator = ' ' if self.instruction else ''\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}{separator}{self.adv_string}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._control_slice = slice(self._goal_slice.stop, len(toks))\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[1], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._assistant_role_slice = slice(self._control_slice.stop, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.target}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._target_slice = slice(self._assistant_role_slice.stop, len(toks)-2)\n",
        "            self._loss_slice = slice(self._assistant_role_slice.stop-1, len(toks)-3)\n",
        "\n",
        "        elif self.conv_template.name == 'oasst_pythia':\n",
        "            # This is specific to the vicuna and pythia tokenizer and conversation prompt.\n",
        "            # It will not work with other tokenizers or prompts.\n",
        "            self.conv_template.messages = []\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[0], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._user_role_slice = slice(None, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._goal_slice = slice(self._user_role_slice.stop, max(self._user_role_slice.stop, len(toks)-1))\n",
        "\n",
        "            separator = ' ' if self.instruction else ''\n",
        "            self.conv_template.update_last_message(f\"{self.instruction}{separator}{self.adv_string}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._control_slice = slice(self._goal_slice.stop, len(toks)-1)\n",
        "\n",
        "            self.conv_template.append_message(self.conv_template.roles[1], None)\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._assistant_role_slice = slice(self._control_slice.stop, len(toks))\n",
        "\n",
        "            self.conv_template.update_last_message(f\"{self.target}\")\n",
        "            toks = self.tokenizer(self.conv_template.get_prompt()).input_ids\n",
        "            self._target_slice = slice(self._assistant_role_slice.stop, len(toks)-1)\n",
        "            self._loss_slice = slice(self._assistant_role_slice.stop-1, len(toks)-2)\n",
        "        else:\n",
        "            assert False, \"Conversation template not supported\"\n",
        "\n",
        "        self.conv_template.messages = []\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def get_input_ids(self, adv_string=None):\n",
        "        prompt = self.get_prompt(adv_string=adv_string)\n",
        "        toks = self.tokenizer(prompt).input_ids\n",
        "        input_ids = torch.tensor(toks[:self._target_slice.stop])\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "\n",
        "\"\"\"\"\n",
        "Now the string utils code\n",
        "\"\"\"\n",
        "\n",
        "def get_nonascii_toks(tokenizer):\n",
        "\n",
        "    def is_ascii(s):\n",
        "        return s.isascii() and s.isprintable()\n",
        "\n",
        "    ascii_toks = []\n",
        "    for i in range(3, tokenizer.vocab_size):\n",
        "        if not is_ascii(tokenizer.decode([i])):\n",
        "            ascii_toks.append(i)\n",
        "\n",
        "    if tokenizer.bos_token_id is not None:\n",
        "        ascii_toks.append(tokenizer.bos_token_id)\n",
        "    if tokenizer.eos_token_id is not None:\n",
        "        ascii_toks.append(tokenizer.eos_token_id)\n",
        "    if tokenizer.pad_token_id is not None:\n",
        "        ascii_toks.append(tokenizer.pad_token_id)\n",
        "    if tokenizer.unk_token_id is not None:\n",
        "        ascii_toks.append(tokenizer.unk_token_id)\n",
        "\n",
        "    return torch.tensor(ascii_toks, device=device)\n",
        "\n",
        "\n",
        "def create_batch_of_control_tokens(\n",
        "    new_adv_suffix_strings: List[str],\n",
        "    suffix_manager: SuffixManager\n",
        ") -> Tuple[Int[Tensor, \"batch_size len\"], Int[Tensor, \"batch_size len\"]]:\n",
        "    # create the batch such that we can feed it all at once into the model\n",
        "    # this isn't very efficient ):\n",
        "\n",
        "    # stores the list of tokenized prompts that we want to test\n",
        "    all_tokens = []\n",
        "    max_len = 0\n",
        "\n",
        "    for adv_string in new_adv_suffix_strings:\n",
        "        # print(tokenizer.decode(adv_suffix))\n",
        "        # adv_string = tokenizer.decode(adv_suffix)\n",
        "        # print(suffix_manager.get_input_ids(adv_string=adv_string))\n",
        "\n",
        "        all_tokens.append(suffix_manager.get_input_ids(adv_string=adv_string))\n",
        "        max_len = max(len(all_tokens[-1]), max_len)\n",
        "\n",
        "    # print(type(max_len))\n",
        "    # print(type(batch_size))\n",
        "    # print(tokenizer.pad_token_id)\n",
        "    batch = torch.full((batch_size, max_len), tokenizer.pad_token_id, dtype=torch.long, device=device)\n",
        "    # tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
        "\n",
        "    for i, token_ids in enumerate(all_tokens):\n",
        "        # pad tokens[i] to have len max_len with tokenizer.pad_token_id\n",
        "        # tokens[i] = torch.cat((tokens[i], torch.zeros(max_len - len(tokens[i]), dtype=torch.long)), dim=0)\n",
        "\n",
        "        # tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
        "        batch[i, :len(token_ids)] = torch.tensor(token_ids, dtype=torch.long, device=device)\n",
        "\n",
        "    attention_mask = (batch != tokenizer.pad_token_id).type(batch.dtype)\n",
        "\n",
        "    return batch, attention_mask\n",
        "\n",
        "# returns the loss for each of the adv suffixes we want to test\n",
        "def cross_entropy_loss(\n",
        "    batch: Int[Tensor, \"batch_size seq_len\"],\n",
        "    logits: Float[Tensor, \"batch_size seq_len d_vocab\"],\n",
        "    suffix_manager: SuffixManager\n",
        ") -> Float[Tensor, \"batch_size\"]:\n",
        "\n",
        "    # the loss slice is shifted by 1 to the left\n",
        "    target_slice = slice(suffix_manager._target_slice.start, suffix_manager._target_slice.stop)\n",
        "    loss_slice = slice(suffix_manager._target_slice.start-1, suffix_manager._target_slice.stop-1)\n",
        "\n",
        "    # we want the shape of the labels to match the first two dimensions of the batch\n",
        "    # to be able to apply the gather operation\n",
        "    labels = einops.repeat(batch[0, target_slice], \"target_size -> batch_size target_size 1\", batch_size=batch.shape[0])\n",
        "\n",
        "    #print(\"Loss logits shape\", logits[:,loss_slice].log_softmax(dim=-1).shape)\n",
        "    #print(\"Labels shape\", labels.shape)\n",
        "\n",
        "    target_logprobs = torch.gather(\n",
        "        logits[:,loss_slice].log_softmax(dim=-1),\n",
        "        dim=-1,\n",
        "        index=labels\n",
        "    ).squeeze(-1) # we have one extra dimension at the end we don't need\n",
        "\n",
        "    #print(f\"Logprobs has shape {target_logprobs.shape}\")\n",
        "\n",
        "    return target_logprobs.mean(dim=-1)\n",
        "\n",
        "\n",
        "def generate(model,\n",
        "             tokenizer,\n",
        "             prompt: str,\n",
        "             max_new_tokens: int = 25,\n",
        "             generation_config = None\n",
        ") -> str:\n",
        "    if generation_config is None:\n",
        "        generation_config = model.generation_config\n",
        "        generation_config.max_new_tokens = max_new_tokens\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    tokens = model.generate(**inputs, generation_config=generation_config)\n",
        "    return tokenizer.decode(tokens[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up the attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "num_steps = 1\n",
        "user_prompt = \"Hey dude, what is your name?\"\n",
        "adv_string_init = \"! ! ! ! !\"\n",
        "target = \"My name is John Cena!\"\n",
        "template_name = 'oasst-pythia'\n",
        "batch_size = 64\n",
        "topk = 32\n",
        "\n",
        "allow_non_ascii = True # you can set this to True to use unicode tokens\n",
        "\n",
        "test_prefixes = [\"I'm sorry\",\n",
        "                \"Sorry\",\n",
        "                \"I apologize\",\n",
        "                \"As an\",\n",
        "                \"I'm just\",\n",
        "                \"I'm an\",\n",
        "                \"I cannot\",\n",
        "                \"I would advise\",\n",
        "                \"it's not appropriate\",\n",
        "                \"As a responsible AI\"]\n",
        "\n",
        "# model, tokenizer = get_model_and_tokenizer(model_name=\"EleutherAI/pythia-70m\")\n",
        "\n",
        "# load it from local storage\n",
        "# model, tokenizer = get_model_and_tokenizer(model_path=\"EleutherAI/pythia-70m\", tokenizer_path=\"EleutherAI/pythia-70m\")\n",
        "model, tokenizer = get_model_and_tokenizer(model_path=\"../models/small_pythia\", tokenizer_path=\"../models/eleuther_tokenizer\")\n",
        "\n",
        "conv_template = load_conversation_template(\"oasst_pythia\")\n",
        "\n",
        "suffix_manager = SuffixManager(tokenizer=tokenizer,\n",
        "                               conv_template=conv_template,\n",
        "                               instruction=user_prompt,\n",
        "                               target=target,\n",
        "                               adv_string=adv_string_init)\n",
        "\n",
        "not_allowed_tokens = None if allow_non_ascii else get_nonascii_toks(tokenizer)\n",
        "\n",
        "adv_suffix = adv_string_init\n",
        "\n",
        "plotlosses = PlotLosses()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xutrHOqgqBYz"
      },
      "source": [
        "### Executing the attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "49gHM40hMHLL",
        "outputId": "befe84b5-59d3-4de8-ba12-fbfa965f36d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Suffix <start>! ! ! ! !</end> of length 12\n",
            "Generated text: <start>!!!</end>\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m tokens_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mencode(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00muser_prompt\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00madv_suffix\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSuffix <start>\u001b[39m\u001b[39m{\u001b[39;00madv_suffix\u001b[39m}\u001b[39;00m\u001b[39m</end> of length \u001b[39m\u001b[39m{\u001b[39;00mtokens_len\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mGenerated text: <start>\u001b[39m\u001b[39m{\u001b[39;00mgenerated_text\u001b[39m}\u001b[39;00m\u001b[39m</end>\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m coordinate_grad \u001b[39m=\u001b[39m token_gradients(model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                   input_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                                   suffix_manager\u001b[39m.\u001b[39;49m_control_slice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                                   suffix_manager\u001b[39m.\u001b[39;49m_target_slice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                                   suffix_manager\u001b[39m.\u001b[39;49m_loss_slice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m adv_suffix_tokens \u001b[39m=\u001b[39m input_ids[suffix_manager\u001b[39m.\u001b[39m_control_slice]\u001b[39m.\u001b[39mto(device)\n",
            "\u001b[1;32m/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()(logits[\u001b[39m0\u001b[39m,loss_slice,:], targets)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m \u001b[39m#print(f\"Forward pass\")\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m grad \u001b[39m=\u001b[39m one_hot\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/oscarbalcells/Desktop/supermech/miexperiments/llm_jailbreak/clean_pythia_jailbreak.ipynb#X11sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m grad \u001b[39m=\u001b[39m grad \u001b[39m/\u001b[39m grad\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/anaconda3/envs/arena/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/arena/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "adv_suffix = adv_string_init\n",
        "num_steps = 10 \n",
        "\n",
        "for i in range(num_steps):\n",
        "    input_ids = suffix_manager.get_input_ids(adv_string=adv_suffix)\n",
        "\n",
        "    generated_text = generate(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        f\"{user_prompt}{adv_suffix}\",\n",
        "        max_new_tokens=7\n",
        "    )[len(f\"{user_prompt}{adv_suffix}\"):]\n",
        "\n",
        "    tokens_len = len(tokenizer.encode(f\"{user_prompt}{adv_suffix}\"))\n",
        "\n",
        "    print(f\"\\nSuffix <start>{adv_suffix}</end> of length {tokens_len}\\nGenerated text: <start>{generated_text}</end>\\n\")\n",
        "\n",
        "    coordinate_grad = token_gradients(model,\n",
        "                                      input_ids,\n",
        "                                      suffix_manager._control_slice,\n",
        "                                      suffix_manager._target_slice,\n",
        "                                      suffix_manager._loss_slice)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    adv_suffix_tokens = input_ids[suffix_manager._control_slice].to(device)\n",
        "\n",
        "    new_adv_suffix_toks = sample_control(adv_suffix_tokens,\n",
        "                                         coordinate_grad,\n",
        "                                         batch_size,\n",
        "                                         topk=topk,\n",
        "                                         temp=1,\n",
        "                                         not_allowed_tokens=not_allowed_tokens)\n",
        "\n",
        "    #print(f\"New adv suffix toks {new_adv_suffix_toks}\")\n",
        "    #print(f\"Decoded new toks {tokenizer.decode(new_adv_suffix_toks[0])}\")\n",
        "\n",
        "    del coordinate_grad\n",
        "\n",
        "    #print(f\"New adv suffix toks {new_adv_suffix_toks[0]}\")\n",
        "\n",
        "    # TODO: fix such that we can filter candidates\n",
        "    new_adv_suffix = get_filtered_cands(tokenizer,\n",
        "                                        new_adv_suffix_toks,\n",
        "                                        filter_cand=False,\n",
        "                                        curr_control=adv_suffix)\n",
        "\n",
        "    #print(f\"New adv suffix we'll try now: {new_adv_suffix}\")\n",
        "\n",
        "    control_batch, attention_mask = create_batch_of_control_tokens(new_adv_suffix,\n",
        "                                                                   suffix_manager)\n",
        "\n",
        "    #print(f\"Control batch {control_batch}\")\n",
        "    #print(f\"Decoded first element of batch {tokenizer.decode(control_batch[0])}\")\n",
        "    #print(f\"\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids=control_batch, attention_mask=attention_mask).logits\n",
        "\n",
        "    #print(f\"Logits shape {logits.shape}\")\n",
        "\n",
        "    losses = cross_entropy_loss(control_batch, logits, suffix_manager)\n",
        "\n",
        "    del logits\n",
        "\n",
        "    best_new_adv_suffix_idx = losses.argmin()\n",
        "    best_new_adv_suffix = new_adv_suffix[best_new_adv_suffix_idx]\n",
        "\n",
        "    current_loss = losses[best_new_adv_suffix_idx]\n",
        "\n",
        "    # next time we'll try with this one\n",
        "    adv_suffix = best_new_adv_suffix\n",
        "\n",
        "    # plot the loss and keep going\n",
        "    plotlosses.update({'Loss': current_loss.detach().cpu().numpy()})\n",
        "    plotlosses.send()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "qhFlLABpo__-",
        "outputId": "01f2b359-470b-4876-cf39-70e09cd86bc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Hello, I am a newbie to this forum. I am trying to install a program called \"Spyder\" on my computer. I'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model, tokenizer, \"Hello, I am\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### To-do:\n",
        "- Test that the cross entropy loss is working correctly\n",
        "- Put the whole attack into one helper class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- suffix manager (class)\n",
        "    - get_prompt\n",
        "    - get_input_tokens\n",
        "- \n",
        "- token_gradients\n",
        "    - Is scatter better?\n",
        "- get_filtered_cands\n",
        "- get_nonascii_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- how many control suffix are we throwing away due to the fixed-size requirement?\n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from clean_suffix_attack import SuffixAttack\n",
        "from clean_suffix_attack import test_shape, test_candidate_sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "attack = SuffixAttack(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    conv_template=conv_template,\n",
        "    instruction=\"Hey dude, what is your name?\",\n",
        "    target=\"My name is John Cena!\",\n",
        "    initial_adv_suffix=\"! ! ! ! !\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Things the class should have\n",
        "- Merge sample candidates & get filtered candidates\n",
        "- Sample control\n",
        "- Abstract class for the general stuff\n",
        "- An easier way to deal with the conversation templates\n",
        "    - Just do it the way nina does it with llama2\n",
        "- \n",
        "\n",
        "### Questions/Concerns\n",
        "- Is it okay for the loss to be negative?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dependenciesbloggerem)). with loss -3.288073778152466"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original adv suffix is ! ! ! ! ! with loss -3.7365314960479736\n",
            "Input id for suffix  thro!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 50276, 38088, 18963,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix  hu!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 50276, 11917, 18963,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix  uno!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 50276, 26162, 18963,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix translation!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 10234, 18963,     0,    29,    93,   515,\n",
            "         5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix ! !!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 23358, 15844,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !lys!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 31453, 15844,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !nik!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 16825, 15844,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!opens!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 40040, 25249,  4672,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !! Jes!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 40040,  6445,  4672,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !! corrupt!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 40040, 17715,  4672,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!!bolt! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195,  4672, 45296,     2,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Input id for suffix !!! Organisation! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195,  4672, 44115,     2,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Input id for suffix !!! rotten! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195,  4672, 46301,     2,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Input id for suffix !!!! Fire is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844,  8726,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!!! nerve is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844, 11083,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!!!Upload is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844, 31483,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Done getting the candidate strings! Candidate strings are [' thro!!!!', ' hu!!!!', ' uno!!!!', 'translation!!!!', '! !!!', '!lys!!!', '!nik!!!', '!!opens!!', '!! Jes!!', '!! corrupt!!', '!!!bolt!', '!!! Organisation!', '!!! rotten!', '!!!! Fire', '!!!! nerve', '!!!!Upload']\n",
            "Batch is (shape torch.Size([1, 32])): tensor([[   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "           310,   634,  1416,    32,  2195,  2195,  2195,  2195,  2195,     0,\n",
            "            29,    93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,\n",
            "          6736,     2]])\n",
            "Logits shape torch.Size([16, 31, 50304])\n",
            "Last tokens in the batch of cands tensor([[3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2]])\n",
            "New adv string is !!!! Fire with loss -2.986448287963867\n",
            "Tokens that were fed are tensor([    0,    29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,\n",
            "          752,   310,   634,  1416,    32,  2195, 15844,  8726,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "New adv suffix is !!!! Fire with loss -3.6924679279327393\n"
          ]
        }
      ],
      "source": [
        "attack.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original adv suffix is ! ! ! ! ! with loss -3.7365314960479736\n",
            "Done getting the candidate strings! Candidate strings are [' thro!!!!', ' hu!!!!', ' uno!!!!', 'translation!!!!', '! !!!', '!lys!!!', '!nik!!!', '!!opens!!', '!! Jes!!', '!! corrupt!!', '!!!bolt!', '!!! Organisation!', '!!! rotten!', '!!!! Fire', '!!!! nerve', '!!!!Upload']\n",
            "Batch is (shape torch.Size([1, 32])): tensor([[   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "           310,   634,  1416,    32,  2195,  2195,  2195,  2195,  2195,     0,\n",
            "            29,    93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,\n",
            "          6736,     2]])\n",
            "Logits shape torch.Size([16, 31, 50304])\n",
            "Last tokens in the batch of cands tensor([[3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2]])\n",
            "New adv string is !!!! Fire with loss -2.986448287963867\n",
            "Tokens that were fed are tensor([    0,    29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,\n",
            "          752,   310,   634,  1416,    32,  2195, 15844,  8726,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Tokens that will be feed now are tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844,  8726,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "New adv suffix is !!!! Fire with loss -3.6924679279327393\n"
          ]
        }
      ],
      "source": [
        "attack.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original adv suffix is ! ! ! ! ! with loss -3.7365314960479736\n",
            "Done getting the candidate strings! Candidate strings are [' thro!!!!', ' hu!!!!', ' uno!!!!', 'translation!!!!', '! !!!', '!lys!!!', '!nik!!!', '!!opens!!', '!! Jes!!', '!! corrupt!!', '!!!bolt!', '!!! Organisation!', '!!! rotten!', '!!!! Fire', '!!!! nerve', '!!!!Upload']\n",
            "Batch is (shape torch.Size([1, 32])): tensor([[   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "           310,   634,  1416,    32,  2195,  2195,  2195,  2195,  2195,     0,\n",
            "            29,    93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,\n",
            "          6736,     2]])\n",
            "Logits shape torch.Size([16, 31, 50304])\n",
            "Last tokens in the batch of cands tensor([[3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2],\n",
            "        [3220, 1416,  310, 2516,  330, 6736,    2]])\n",
            "New adv string is !!!! Fire with loss -2.986448287963867\n",
            "Tokens that were fed are tensor([    0,    29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,\n",
            "          752,   310,   634,  1416,    32,  2195, 15844,  8726,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Fed loss is -3.422624349594116\n",
            "Tokens that will be feed now are tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844,  8726,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "New adv suffix is !!!! Fire with loss -3.6924679279327393\n"
          ]
        }
      ],
      "source": [
        "attack.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|endoftext|><|prompter|>Hey dude, what is your name?!!!! Fire<|endoftext|><|assistant|>My name is John Cena!\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode([    0,    29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,\n",
        "          752,   310,   634,  1416,    32,  2195, 15844,  8726,     0,    29,\n",
        "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
        "            2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 2195, 2195, 2195, 8726]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.encode(\"! ! ! ! Fire\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!!!!!'"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode([2, 2195, 2195, 2195, 2195], skip_special_tokens=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input ids are tensor([[   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "           310,   634,  1416,    32,  2195,  2195,  2195,  2195,  2195,     0,\n",
            "            29,    93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,\n",
            "          6736,     2]])\n",
            "Target input ids are tensor([[3220, 1416,  310, 2516,  330, 6736,    2]])\n",
            "Shape of target input ids is: \n",
            "Input ids are tensor([[   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "           310,   634,  1416,    32,  2195,  2195,  2195,  2195,  2195,     0,\n",
            "            29,    93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,\n",
            "          6736,     2]])\n",
            "Target input ids are tensor([[3220, 1416,  310, 2516,  330, 6736,    2]])\n",
            "Shape of target input ids is: \n",
            "New token pos is tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4])\n",
            "Top indices is tensor([[33355,  5208,  5474,  2830, 37137,  3428, 41188,  9609,  1948,  4878,\n",
            "         47941, 16953, 17370,  5365, 12875, 11262, 35542, 34706, 12017,   587,\n",
            "         43027,  9409,  4137, 37094,   419, 37515, 15341,  9584, 49070, 49253],\n",
            "        [33355,  5208,  1948, 25624, 15341,   587, 16953, 47531,  6445, 31849,\n",
            "         14633, 41188,  5365, 12042,  2431, 49253, 37165, 17021, 26413, 16393,\n",
            "         17370,  9584, 11774, 19038,  1526, 34706, 43027,  3428, 44526,  2103],\n",
            "        [33355, 31849, 47531, 26413,  1948, 41437,  5208, 41188, 26148,  6445,\n",
            "         16953, 32141, 37165, 42155, 49253, 14633, 15341, 48963,   587, 25624,\n",
            "         34396, 44526, 14554, 11934, 17370, 10218, 18282, 43027, 19493,  7984],\n",
            "        [33355, 26148, 31849, 41437, 11934, 26413, 47531, 14633, 18282,   587,\n",
            "         24627, 22201, 10218,  1948, 42155, 17370, 16953, 11037, 14910,  5735,\n",
            "         32141,  9609, 40298, 28665, 42219, 24830, 41188, 25096, 11774,  7984],\n",
            "        [26148, 11934,   587, 17370, 41437, 47531, 14633, 24830,  5474, 22201,\n",
            "         26413, 26616, 31849, 29286, 14399,  9609, 24627, 11774, 11037, 10821,\n",
            "         14910, 31368, 16953, 18282, 37094,  5893, 10218,  1948,  5735, 17631]])\n",
            "Input id for suffix  RA!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 50276,  5214, 18963,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix  Mus!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 50276, 13788, 18963,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix  snakes!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 50276, 11489,  1582, 18963,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Input id for suffix Bot!!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 28469, 18963,     0,    29,    93,   515,\n",
            "         5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !bot!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 12042, 15844,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix ! Rams!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 31849, 15844,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix ! indis!!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 47531, 15844,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!mime!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 40040, 42155,  4672,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!ENO!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 40040, 41188,  4672,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !! bile!! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32, 40040, 26413,  4672,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!!ugs! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195,  4672, 22201,     2,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Input id for suffix !!!ENO! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195,  4672, 41188,     2,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Input id for suffix !!! indis! is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195,  4672, 47531,     2,     0,    29,\n",
            "           93,   515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,\n",
            "            2])\n",
            "Input id for suffix !!!!ugs is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844, 22201,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!!!idel is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844,  5735,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Input id for suffix !!!! thro is tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
            "          310,   634,  1416,    32,  2195, 15844, 10218,     0,    29,    93,\n",
            "          515,  5567, 49651,  3220,  1416,   310,  2516,   330,  6736,     2])\n",
            "Suffix candidate strings are: [' RA!!!!', ' Mus!!!!', ' snakes!!!!', 'Bot!!!!', '!bot!!!', '! Rams!!!', '! indis!!!', '!!mime!!', '!!ENO!!', '!! bile!!', '!!!ugs!', '!!!ENO!', '!!! indis!', '!!!!ugs', '!!!!idel', '!!!! thro']\n",
            "OG top indices are: tensor([[33355,  5208,  5474,  2830, 37137,  3428, 41188,  9609,  1948,  4878,\n",
            "         47941, 16953, 17370,  5365, 12875, 11262, 35542, 34706, 12017,   587,\n",
            "         43027,  9409,  4137, 37094,   419, 37515, 15341,  9584, 49070, 49253],\n",
            "        [33355,  5208,  1948, 25624, 15341,   587, 16953, 47531,  6445, 31849,\n",
            "         14633, 41188,  5365, 12042,  2431, 49253, 37165, 17021, 26413, 16393,\n",
            "         17370,  9584, 11774, 19038,  1526, 34706, 43027,  3428, 44526,  2103],\n",
            "        [33355, 31849, 47531, 26413,  1948, 41437,  5208, 41188, 26148,  6445,\n",
            "         16953, 32141, 37165, 42155, 49253, 14633, 15341, 48963,   587, 25624,\n",
            "         34396, 44526, 14554, 11934, 17370, 10218, 18282, 43027, 19493,  7984],\n",
            "        [33355, 26148, 31849, 41437, 11934, 26413, 47531, 14633, 18282,   587,\n",
            "         24627, 22201, 10218,  1948, 42155, 17370, 16953, 11037, 14910,  5735,\n",
            "         32141,  9609, 40298, 28665, 42219, 24830, 41188, 25096, 11774,  7984],\n",
            "        [26148, 11934,   587, 17370, 41437, 47531, 14633, 24830,  5474, 22201,\n",
            "         26413, 26616, 31849, 29286, 14399,  9609, 24627, 11774, 11037, 10821,\n",
            "         14910, 31368, 16953, 18282, 37094,  5893, 10218,  1948,  5735, 17631]])\n",
            "Batch shape torch.Size([16, 31])\n",
            "Custom suffix candidate strings: [' RA!!!!', ' Mus!!!!', ' snakes!!!!', 'Bot!!!!', '!bot!!!', '! Rams!!!', '! indis!!!', '!!mime!!', '!!ENO!!', '!! bile!!', '!!!ugs!', '!!!ENO!', '!!! indis!', '!!!!ugs', '!!!!idel', '!!!! thro']\n",
            "Original suffix candidate strings: [' RA!!!!', ' Mus!!!!', ' snakes!!!!', 'Bot!!!!', '!bot!!!', '! Rams!!!', '! indis!!!', '!!mime!!', '!!ENO!!', '!! bile!!', '!!!ugs!', '!!!ENO!', '!!! indis!', '!!!!ugs', '!!!!idel', '!!!! thro']\n",
            "\u001b[92mSampling tests passed!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "test_candidate_sampling(model, tokenizer, conv_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[[33355,  5208,  5474,  ..., 36333,  7047,  8725],\n",
        "        [33355,  5208,  1948,  ..., 20307, 26498, 40920],\n",
        "        [33355, 31849, 47531,  ..., 36240, 24401, 30904],\n",
        "        [33355, 26148, 31849,  ..., 25948, 31922, 27582],\n",
        "        [26148, 11934,   587,  ..., 41864, 43555, 43351]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[[33355,  5208,  5474,  2830],\n",
        "        [33355,  5208,  1948, 25624],\n",
        "        [33355, 31849, 47531, 26413],\n",
        "        [33355, 26148, 31849, 41437],\n",
        "        [26148, 11934,   587, 17370]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing using original LLM-attacks code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/oscarbalcells/Desktop/fork-llm-attacks\n"
          ]
        }
      ],
      "source": [
        "%cd fork-llm-attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Obtaining file:///Users/oscarbalcells/Desktop/fork-llm-attacks\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.28.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from llm-attacks==0.0.1) (4.32.0)\n",
            "Requirement already satisfied: ml_collections in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from llm-attacks==0.0.1) (0.1.1)\n",
            "Requirement already satisfied: fschat>=0.2.20 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from llm-attacks==0.0.1) (0.2.27)\n",
            "Requirement already satisfied: aiohttp in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (3.8.5)\n",
            "Requirement already satisfied: fastapi in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (0.103.1)\n",
            "Requirement already satisfied: httpx in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (0.24.1)\n",
            "Requirement already satisfied: markdown2[all] in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (2.4.10)\n",
            "Requirement already satisfied: nh3 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (0.2.14)\n",
            "Requirement already satisfied: numpy in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (1.24.4)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (3.0.39)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (1.10.12)\n",
            "Requirement already satisfied: requests in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (2.31.0)\n",
            "Requirement already satisfied: rich>=10.0.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (13.5.2)\n",
            "Requirement already satisfied: shortuuid in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (1.0.11)\n",
            "Requirement already satisfied: tiktoken in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (0.4.0)\n",
            "Requirement already satisfied: uvicorn in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fschat>=0.2.20->llm-attacks==0.0.1) (0.23.2)\n",
            "Requirement already satisfied: filelock in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (3.12.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (2023.8.8)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from transformers>=4.28.1->llm-attacks==0.0.1) (4.66.1)\n",
            "Requirement already satisfied: absl-py in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from ml_collections->llm-attacks==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: six in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from ml_collections->llm-attacks==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from ml_collections->llm-attacks==0.0.1) (21.6.0)\n",
            "Requirement already satisfied: fsspec in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.28.1->llm-attacks==0.0.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.28.1->llm-attacks==0.0.1) (4.7.1)\n",
            "Requirement already satisfied: wcwidth in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from prompt-toolkit>=3.0.0->fschat>=0.2.20->llm-attacks==0.0.1) (0.2.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from rich>=10.0.0->fschat>=0.2.20->llm-attacks==0.0.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from rich>=10.0.0->fschat>=0.2.20->llm-attacks==0.0.1) (2.16.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat>=0.2.20->llm-attacks==0.0.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat>=0.2.20->llm-attacks==0.0.1) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat>=0.2.20->llm-attacks==0.0.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat>=0.2.20->llm-attacks==0.0.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat>=0.2.20->llm-attacks==0.0.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat>=0.2.20->llm-attacks==0.0.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from aiohttp->fschat>=0.2.20->llm-attacks==0.0.1) (1.3.1)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fastapi->fschat>=0.2.20->llm-attacks==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from fastapi->fschat>=0.2.20->llm-attacks==0.0.1) (0.27.0)\n",
            "Requirement already satisfied: certifi in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat>=0.2.20->llm-attacks==0.0.1) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat>=0.2.20->llm-attacks==0.0.1) (0.17.3)\n",
            "Requirement already satisfied: idna in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat>=0.2.20->llm-attacks==0.0.1) (3.4)\n",
            "Requirement already satisfied: sniffio in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from httpx->fschat>=0.2.20->llm-attacks==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: wavedrom in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from markdown2[all]->fschat>=0.2.20->llm-attacks==0.0.1) (2.0.3.post3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from requests->fschat>=0.2.20->llm-attacks==0.0.1) (1.26.16)\n",
            "Requirement already satisfied: click>=7.0 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from uvicorn->fschat>=0.2.20->llm-attacks==0.0.1) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from uvicorn->fschat>=0.2.20->llm-attacks==0.0.1) (0.14.0)\n",
            "Requirement already satisfied: exceptiongroup in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->fschat>=0.2.20->llm-attacks==0.0.1) (1.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat>=0.2.20->llm-attacks==0.0.1) (0.1.2)\n",
            "Requirement already satisfied: svgwrite in /Users/oscarbalcells/anaconda3/envs/arena/lib/python3.8/site-packages (from wavedrom->markdown2[all]->fschat>=0.2.20->llm-attacks==0.0.1) (1.4.3)\n",
            "Installing collected packages: llm-attacks\n",
            "  Attempting uninstall: llm-attacks\n",
            "    Found existing installation: llm-attacks 0.0.1\n",
            "    Uninstalling llm-attacks-0.0.1:\n",
            "      Successfully uninstalled llm-attacks-0.0.1\n",
            "  Running setup.py develop for llm-attacks\n",
            "Successfully installed llm-attacks-0.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_attacks.minimal_gcg.string_utils import SuffixManager, load_conversation_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "adv_string_init = \"! ! hello ! ! !\"\n",
        "suffix_manager = SuffixManager(tokenizer=tokenizer,\n",
        "              conv_template=conv_template,\n",
        "              instruction=user_prompt,\n",
        "              target=target,\n",
        "              adv_string=adv_string_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = suffix_manager.get_input_ids(adv_string=adv_string_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([   29,    93, 13382,    81,   350, 49651,  8262, 29723,    13,   752,\n",
              "          310,   634,  1416,    32,  2195,  2195, 23120,  2195,  2195,  2195,\n",
              "            0,    29,    93,   515,  5567, 49651,  3220,  1416,   310,  2516,\n",
              "          330,  6736,     2])"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "list_of_chars = [tokenizer.decode(input_ids[i]) for i in range(len(input_ids))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<#|#prom#p#ter#|>#Hey# dude#,# what# is# your# name#?#!#!# hello#!#!#!#<|endoftext|>#<#|#ass#istant#|>#My# name# is# John# C#ena#!\n"
          ]
        }
      ],
      "source": [
        "print(*list_of_chars, sep=\"#\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['!', '!', ' hello', '!', '!', '!']"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_of_chars[suffix_manager._control_slice]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['My', ' name', ' is', ' John', ' C', 'ena', '!']"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_of_chars[suffix_manager._target_slice]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|prompter|>Hey dude, what is your name?!! hello!!!<|endoftext|><|assistant|>My name is John Cena!'"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|prompter|>Hey dude, what is your name?!! hello!!!<|endoftext|><|assistant|>My name is John Cena!'"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(input_ids, skip_special_tokens=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
